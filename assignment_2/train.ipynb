{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e800c4f2-1a29-4b72-9596-cb62b2adf468",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77f5c1ed-b614-4f3f-9ebf-50d2b80f736e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, random\n",
    "import numpy as np, pandas as pd, matplotlib as mpl, matplotlib.pyplot as plt, seaborn as sns\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Tuple, List\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79cf19d4-66fb-4455-84b6-905f884403cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot style from IKT215, dpi set to 200\n",
    "def set_mpl_params(dpi: int = 200, figsize: Tuple[int, int] = (9, 6), grid: bool = True, font_size: int = 12, font_family: str = 'serif') -> None:\n",
    "    mpl.rcParams['figure.dpi'] = dpi\n",
    "    mpl.rcParams['figure.figsize'] = figsize\n",
    "    mpl.rcParams['axes.grid'] = grid\n",
    "    mpl.rcParams.update({'font.size': font_size})\n",
    "    mpl.rcParams['font.family'] = font_family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db018c3b-2a1b-4b48-8ff0-adb623c4bd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproducilbity\n",
    "def seed_everything(seed: int = 42) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e635001-3aa6-4f6e-a699-6fc7ef15fffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams given in the assignment text, dataclass decorator used for cleaner setup\n",
    "@dataclass\n",
    "class Config:\n",
    "    model_name: str = \"distilbert-base-uncased\"\n",
    "    batch_size: int = 16\n",
    "    grad_accum_steps: int = 2\n",
    "    epochs: int = 10\n",
    "    lr: float = 0.00002\n",
    "    weight_decay: float = 0.01\n",
    "    max_len: int = 512\n",
    "    val_split: float = 0.2\n",
    "    seed: int = 42\n",
    "    warmup_ratio: float = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9368680b-fcd9-4bc9-9a7d-ef07908d5137",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config()\n",
    "set_mpl_params()\n",
    "seed_everything(cfg.seed)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3db325b7-2ef2-43f4-ab37-6be6093dfe25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(\"Using device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6f79da0-cbd7-4e5a-8a13-6485d0944398",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"dataset.parquet\")\n",
    "assert {\"title\", \"content\", \"label\"}.issubset(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad54047e-90c3-498a-9ec8-e9e1bac24f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "df[\"label_id\"] = le.fit_transform(df[\"label\"])\n",
    "# converting np.int64 in dicts to plain python types due to issues with data type handling\n",
    "# str(lbl) makes it so json can serialize labels, and int(i) makes it so the ids are int instead\n",
    "label2id = {str(lbl): int(i) for lbl, i in zip(le.classes_, range(len(le.classes_)))}\n",
    "id2label = {int(i): str(lbl) for lbl, i in enumerate(le.classes_)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed7b16c5-1844-4737-9cfa-303141f8ca57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 84000, validation size: 28000, test size: 28000\n"
     ]
    }
   ],
   "source": [
    "# split into train, validation, and test (0.6 / 0.2 / 0.2)\n",
    "train_df, temp_df = train_test_split(df, test_size = 0.4, stratify = df[\"label_id\"], random_state = cfg.seed)\n",
    "val_df, test_df = train_test_split(temp_df, test_size = 0.5, stratify = temp_df[\"label_id\"], random_state = cfg.seed)\n",
    "print(f\"train size: {len(train_df)}, validation size: {len(val_df)}, test size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8686583-c787-4c90-bff2-382644c98b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "label distribution in full dataset:\n",
      "label\n",
      "4     0.071\n",
      "13    0.071\n",
      "5     0.071\n",
      "9     0.071\n",
      "2     0.071\n",
      "7     0.071\n",
      "0     0.071\n",
      "12    0.071\n",
      "3     0.071\n",
      "6     0.071\n",
      "8     0.071\n",
      "10    0.071\n",
      "11    0.071\n",
      "1     0.071\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "label distribution in splits:\n",
      "train: label\n",
      "1     0.071\n",
      "6     0.071\n",
      "3     0.071\n",
      "12    0.071\n",
      "11    0.071\n",
      "13    0.071\n",
      "5     0.071\n",
      "8     0.071\n",
      "2     0.071\n",
      "9     0.071\n",
      "4     0.071\n",
      "7     0.071\n",
      "10    0.071\n",
      "0     0.071\n",
      "Name: proportion, dtype: float64\n",
      "val:   label\n",
      "12    0.071\n",
      "9     0.071\n",
      "8     0.071\n",
      "10    0.071\n",
      "0     0.071\n",
      "4     0.071\n",
      "6     0.071\n",
      "11    0.071\n",
      "5     0.071\n",
      "3     0.071\n",
      "13    0.071\n",
      "1     0.071\n",
      "2     0.071\n",
      "7     0.071\n",
      "Name: proportion, dtype: float64\n",
      "test:  label\n",
      "8     0.071\n",
      "4     0.071\n",
      "7     0.071\n",
      "6     0.071\n",
      "2     0.071\n",
      "1     0.071\n",
      "0     0.071\n",
      "5     0.071\n",
      "12    0.071\n",
      "13    0.071\n",
      "10    0.071\n",
      "9     0.071\n",
      "3     0.071\n",
      "11    0.071\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# verify label distribution consistency\n",
    "print(\"\\nlabel distribution in full dataset:\")\n",
    "print(df['label'].value_counts(normalize = True).round(3))\n",
    "\n",
    "print(\"\\nlabel distribution in splits:\")\n",
    "print(\"train:\", train_df['label'].value_counts(normalize = True).round(3).head(14))\n",
    "print(\"val:  \", val_df['label'].value_counts(normalize = True).round(3).head(14))\n",
    "print(\"test: \", test_df['label'].value_counts(normalize = True).round(3).head(14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "274456a2-d0bd-4414-9781-fecfd8470bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBpediaDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, tokenizer: DistilBertTokenizer, max_length: int = 512):\n",
    "        # combine title and content into a single text input\n",
    "        # title provides context and content has the detail, joining both improves the coverage\n",
    "        # \"[SEP]\" added to help the model distinguish sections\n",
    "        self.texts = [f\"title: {t} [SEP] content: {c}\" for t, c in zip(df[\"title\"], df[\"content\"])]\n",
    "        self.labels = df[\"label_id\"].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "        text = str(self.texts[idx]) if self.texts[idx] is not None else \"\"\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(text, padding = \"max_length\", truncation = True, max_length = self.max_length, return_tensors = \"pt\")\n",
    "        return {\"input_ids\": encoding[\"input_ids\"].squeeze(0), \"attention_mask\": encoding[\"attention_mask\"].squeeze(0), \"labels\": torch.tensor(label, dtype = torch.long)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fe8b70b-9a04-455e-a9d3-49d590551bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained(cfg.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ffb28ed-a23e-43dc-a5ca-b82cec7b99ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = DBpediaDataset(train_df, tokenizer, cfg.max_len)\n",
    "val_ds = DBpediaDataset(val_df, tokenizer, cfg.max_len)\n",
    "test_ds = DBpediaDataset(test_df, tokenizer, cfg.max_len)\n",
    "train_loader = DataLoader(train_ds, batch_size = cfg.batch_size, shuffle = True, num_workers = 2, pin_memory = torch.cuda.is_available())\n",
    "val_loader = DataLoader(val_ds, batch_size = cfg.batch_size, shuffle = False, num_workers = 2, pin_memory = torch.cuda.is_available())\n",
    "test_loader = DataLoader(test_ds, batch_size = cfg.batch_size, shuffle = False, num_workers = 2, pin_memory = torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25443a1c-0800-41c9-ac0e-bd02a76e6d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: torch.Size([16, 512])\n",
      "attention_mask shape: torch.Size([16, 512])\n",
      "labels shape: torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "# verify tokenized tensor shapes\n",
    "sample = next(iter(train_loader))\n",
    "print(f\"input_ids shape: {sample['input_ids'].shape}\")\n",
    "print(f\"attention_mask shape: {sample['attention_mask'].shape}\")\n",
    "print(f\"labels shape: {sample['labels'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "046914a5-3ca5-45fd-b070-54d1f94d9103",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained(cfg.model_name, num_labels = 14, id2label = id2label, label2id = label2id).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa9d67ba-2f93-47ca-8bb5-85ce00669f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "109f3d36-5275-45c9-a280-780da4a2c7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_layers': 6, 'hidden_size': 768, 'ffn_dim': 3072, 'activation': 'gelu', 'n_heads': 12}\n"
     ]
    }
   ],
   "source": [
    "print({\"n_layers\": model_config.n_layers, \"hidden_size\": model_config.dim, \"ffn_dim\": model_config.hidden_dim, \"activation\": model_config.activation, \"n_heads\": model_config.n_heads})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d769b5a-4a81-4473-b563-49e065ba2950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_weights(model: DistilBertForSequenceClassification, tokenizer: DistilBertTokenizer, text: str, layer_idx: int = 5) -> Tuple[np.ndarray, List[str]]:\n",
    "    inputs = tokenizer(text, return_tensors = 'pt', truncation = True, padding = True, max_length = 512).to(DEVICE)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.base_model(**inputs, output_attentions=True)\n",
    "    attentions = outputs.attentions\n",
    "    layer_attention = attentions[layer_idx][0].detach().cpu().numpy()\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0].cpu())\n",
    "    return layer_attention, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1da38dff-bd75-4125-aa28-eda62c715a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_heatmap(attn: np.ndarray, tokens: List[str], title: str, path: str) -> None:\n",
    "    set_mpl_params()\n",
    "    plt.figure(dpi=200)\n",
    "    plt.imshow(attn, interpolation=\"nearest\", aspect=\"auto\")\n",
    "    plt.xticks(range(len(tokens)), tokens, rotation=90)\n",
    "    plt.yticks(range(len(tokens)), tokens)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=200)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71431ccc-d558-40d5-ac80-93f0c870f7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"A robot may not injure a human being or, through inaction, allow a human being to come to harm.\"\n",
    "# get middle transformer block and one-third into the head set (to avoid extremes)\n",
    "LAYER, HEAD = model.config.n_layers // 2, model.config.n_heads // 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "afe247e3-1fff-4c66-8b77-7af2385c60b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DistilBertSdpaAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    }
   ],
   "source": [
    "pre_attn, pre_tokens = get_attention_weights(model, tokenizer, sentence, LAYER)\n",
    "plot_attention_heatmap(pre_attn[HEAD], pre_tokens, f\"Pre-training layer {LAYER}, head {HEAD}\", \"graphs/attn_pre_single.png\")\n",
    "plot_attention_heatmap(pre_attn.mean(axis=0), pre_tokens, f\"Pre-training layer {LAYER} (Average heads)\", \"graphs/attn_pre_avg.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "87385cbd-4bae-4bc6-a716-4f259e32541b",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_lengths = [len(tokenizer.encode(f\"title: {t} [SEP] content: {c}\", truncation=False)) for t, c in zip(df[\"title\"].head(2000), df[\"content\"].head(2000))]\n",
    "\n",
    "plt.figure(dpi=200)\n",
    "plt.hist(raw_lengths, bins=40, alpha=0.8)\n",
    "plt.axvline(512, linestyle='--', linewidth=1.5, label='max length (512)')\n",
    "plt.title(\"Distribution of raw tokenized sequence lengths (first 2000 samples)\")\n",
    "plt.xlabel(\"Sequence length (tokens)\"); plt.ylabel(\"count\"); plt.legend(); plt.tight_layout()\n",
    "plt.savefig(\"graphs/token_length_distribution.png\", dpi=200); plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b3f700e-1f2a-48b5-b51f-85b1d01136d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr = cfg.lr, weight_decay = cfg.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e4ebca7-57af-4dfb-90e9-db65c426dc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting hyperparams from config to calculate total steps and warmup steps\n",
    "total_train_batches = math.ceil(len(train_ds) / cfg.batch_size)\n",
    "total_steps = total_train_batches * cfg.epochs\n",
    "warmup_steps = int(cfg.warmup_ratio * total_steps)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = warmup_steps, num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5e46902d-788e-41c2-9898-f4cc9facd4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total steps: 52500, Warmup steps: 5250\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total steps: {total_steps}, Warmup steps: {warmup_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cbe239a0-392c-468c-80de-7a6b449b221e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model: DistilBertForSequenceClassification, dataloader: DataLoader, optimizer: torch.optim.Optimizer, scheduler: torch.optim.lr_scheduler.LambdaLR, device: torch.device, grad_accum_steps: int = 1) -> float:\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    progress_bar = tqdm(total = len(dataloader), desc = \"Training\", leave = False)\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        outputs = model(input_ids = input_ids, attention_mask = attention_mask, labels = labels)\n",
    "        loss = outputs.loss / grad_accum_steps\n",
    "        loss.backward()\n",
    "        total_loss += loss.item() * grad_accum_steps\n",
    "        if (step + 1) % grad_accum_steps == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad(set_to_none = True)\n",
    "        if (step + 1) % 500 == 0 or (step + 1) == len(dataloader):\n",
    "            progress_bar.update(500 if (step + 1) % 500 == 0 else len(dataloader) % 500)\n",
    "            progress_bar.set_postfix({\"loss\": f\"{loss.item() * grad_accum_steps:.4f}\"})\n",
    "    progress_bar.close()\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d51cf544-916a-4d97-b133-ab1152b84b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: DistilBertForSequenceClassification, dataloader: DataLoader, device: torch.device) -> Tuple[float, float]:\n",
    "    model.eval()\n",
    "    preds, labels_list = [], []\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            outputs = model(input_ids = input_ids, attention_mask = attention_mask, labels = labels)\n",
    "            total_loss += outputs.loss.item()\n",
    "            preds.extend(outputs.logits.argmax(dim = 1).cpu().numpy())\n",
    "            labels_list.extend(labels.cpu().numpy())\n",
    "    acc = accuracy_score(labels_list, preds)\n",
    "    return acc, total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a85dcba-2051-4cb7-826f-58b047939928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39ed9dd9ae0343b8b955ae7850ae3a1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/5250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best_model.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/5250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best_model.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/5250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best_model.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/5250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/5250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/5250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best_model.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/5250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best_model.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/5250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best_model.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/5250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/5250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_val_acc = 0.0\n",
    "train_losses, val_losses, val_accs = [], [], []\n",
    "\n",
    "epoch_bar = tqdm(range(1, cfg.epochs + 1), desc=\"Training epochs\")\n",
    "for epoch in epoch_bar:\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, scheduler, DEVICE, cfg.grad_accum_steps)\n",
    "    val_acc, val_loss = evaluate(model, val_loader, DEVICE)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    epoch_bar.set_postfix({\"train_loss\": f\"{train_loss:.4f}\", \"val_loss\": f\"{val_loss:.4f}\", \"val_acc\": f\"{val_acc:.4f}\"})\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "        tqdm.write(\"Saved best_model.pt\")\n",
    "epoch_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "88b802c2-a3ec-44f3-b33f-f37488602903",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "best_model = DistilBertForSequenceClassification.from_pretrained(cfg.model_name, num_labels =14, id2label = id2label, label2id = label2id).to(DEVICE)\n",
    "best_model.load_state_dict(torch.load(\"best_model.pt\", map_location = DEVICE))\n",
    "post_attn, post_tokens = get_attention_weights(best_model, tokenizer, sentence, LAYER)\n",
    "plot_attention_heatmap(post_attn[HEAD], post_tokens, f\"Post-training layer {LAYER}, head {HEAD}\", \"graphs/attn_post_single.png\")\n",
    "plot_attention_heatmap(post_attn.mean(axis=0), post_tokens, f\"Post-training layer {LAYER} (Average heads)\", \"graphs/attn_post_avg.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "91cb2d0a-cca6-477a-9233-d54656e978e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi = 200)\n",
    "plt.plot(train_losses, label = \"Training loss\")\n",
    "plt.plot(val_losses, label = \"Validation loss\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.legend(); plt.tight_layout()\n",
    "plt.savefig(\"graphs/loss_curve.png\", dpi = 200); plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e31def2f-a216-496a-bdfd-1331fc6b5f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi = 200)\n",
    "plt.plot(val_accs, label = \"Validation accuracy\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.legend(); plt.tight_layout()\n",
    "plt.savefig(\"graphs/accuracy_curve.png\", dpi = 200); plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1f006b0a-100b-4d86-808c-5ec57583d778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=14, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "best_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7f949586-47d8-4e99-92b5-041c81200250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "testing:   0%|          | 0/1750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc = \"testing\", leave = False):\n",
    "        ids, mask, labels = batch[\"input_ids\"].to(DEVICE), batch[\"attention_mask\"].to(DEVICE), batch[\"labels\"].to(DEVICE)\n",
    "        outputs = best_model(input_ids = ids, attention_mask = mask)\n",
    "        preds = outputs.logits.argmax(dim = 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f381f49d-d61c-4c1b-b616-f79bdd52436d",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(all_labels, all_preds)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average = None, zero_division = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "313941c3-21b2-4351-a5ce-2aea2195948f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize = (9, 7), dpi = 200)\n",
    "sns.heatmap(cm, annot = True, fmt = \"d\", cmap = \"Blues_r\", xticklabels = id2label.values(), yticklabels = id2label.values(), cbar = False, linewidths = 0.4, linecolor = \"gray\")\n",
    "plt.title(\"Confusion matrix - test set\", pad = 12)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.xticks(rotation = 45, ha = \"right\", fontsize = 8)\n",
    "plt.yticks(rotation = 0, fontsize = 8)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"graphs/confusion_matrix_test.png\", dpi = 200, bbox_inches = \"tight\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5ed0fc9d-1ae0-4ef4-91ce-bb5c5a812f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"Company\", \"EducationalInstitution\", \"Artist\", \"Athlete\", \"OfficeHolder\", \"MeanOfTransportation\", \"Building\", \"NaturalPlace\", \"Village\", \"Animal\", \"Plant\", \"Album\", \"Film\", \"WrittenWork\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fe7902c3-c111-4ebe-ad73-b2d6ba5efb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_precision = np.mean(precision)\n",
    "avg_recall = np.mean(recall)\n",
    "avg_f1 = np.mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eddb492e-87db-4cb0-9cae-4dbd9cdb66f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Per-class performance metrics\n",
      "---------------------------------------------------------------------------\n",
      "Class                       Precision      Recall    F1 Score\n",
      "---------------------------------------------------------------------------\n",
      "Company                        0.9783      0.9715      0.9749\n",
      "EducationalInstitution         0.9866      0.9920      0.9893\n",
      "Artist                         0.9830      0.9840      0.9835\n",
      "Athlete                        0.9970      0.9960      0.9965\n",
      "OfficeHolder                   0.9835      0.9835      0.9835\n",
      "MeanOfTransportation           0.9920      0.9940      0.9930\n",
      "Building                       0.9824      0.9770      0.9797\n",
      "NaturalPlace                   0.9925      0.9980      0.9953\n",
      "Village                        0.9980      0.9990      0.9985\n",
      "Animal                         0.9965      0.9970      0.9968\n",
      "Plant                          0.9965      0.9935      0.9950\n",
      "Album                          0.9950      0.9975      0.9963\n",
      "Film                           0.9975      0.9950      0.9962\n",
      "WrittenWork                    0.9940      0.9950      0.9945\n",
      "---------------------------------------------------------------------------\n",
      "Macro average                  0.9909      0.9909      0.9909\n",
      "Overall cccuracy:              0.9909\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPer-class performance metrics\")\n",
    "print(\"-\" * 75)\n",
    "print(f\"{'Class':<25}{'Precision':>12}{'Recall':>12}{'F1 Score':>12}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for c, p, r, f in zip(classes, precision, recall, f1):\n",
    "    print(f\"{c:<25}{p:>12.4f}{r:>12.4f}{f:>12.4f}\")\n",
    "\n",
    "print(\"-\" * 75)\n",
    "print(f\"{'Macro average':<25}{avg_precision:>12.4f}{avg_recall:>12.4f}{avg_f1:>12.4f}\")\n",
    "print(f\"{'Overall cccuracy:':<25}{acc:>12.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
