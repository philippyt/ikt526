{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d255f2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "import matplotlib as mpl\n",
    "import math\n",
    "import textwrap\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from diffusers import StableDiffusionPipeline, UNet2DConditionModel, DDPMScheduler, AutoencoderKL\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as T\n",
    "import re, time, json, random, gc, functools\n",
    "from dataclasses import dataclass, asdict\n",
    "from pathlib import Path\n",
    "from torchmetrics.functional.multimodal import clip_score\n",
    "from functools import partial\n",
    "from PIL import Image\n",
    "from typing import Dict, List, Tuple, Iterable, Any, Optional\n",
    "import pathlib, importlib\n",
    "from tqdm.auto import tqdm\n",
    "from diffusers.optimization import get_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276c17ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot style from IKT215\n",
    "def set_mpl_params(dpi: int = 450, figsize: Tuple[int, int] = (9, 6), grid: bool = True, font_size: int = 12, font_family: str = 'serif') -> None:\n",
    "    mpl.rcParams['figure.dpi'] = dpi\n",
    "    mpl.rcParams['figure.figsize'] = figsize\n",
    "    mpl.rcParams['axes.grid'] = grid\n",
    "    mpl.rcParams.update({'font.size': font_size})\n",
    "    mpl.rcParams['font.family'] = font_family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164fe737",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Norod78/cartoon-blip-captions\", split = \"train\")\n",
    "num_samples = 1500\n",
    "indices = random.sample(range(len(dataset)), num_samples)\n",
    "dataset = dataset.select(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0929d5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(dataset, samples: int = 15, cols: int = 4) -> None:\n",
    "    set_mpl_params(figsize = (15, 15))\n",
    "    rows = math.ceil(samples / cols)\n",
    "    for i in range(samples):\n",
    "        img = dataset[i]['image']\n",
    "        caption = dataset[i]['text']\n",
    "        wrapped = \"\\n\".join(textwrap.wrap(caption, width = 40))\n",
    "        plt.subplot(rows, cols, i + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(wrapped)\n",
    "        plt.axis('off')\n",
    "        plt.title(wrapped, fontsize = 8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43f6e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb6acab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset)\n",
    "print(dataset.features)\n",
    "print(len(dataset))\n",
    "if len(dataset) > 500:\n",
    "    print(\"good to go\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f44abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_device() -> tuple[torch.device, bool]:\n",
    "    if torch.cuda.is_available():\n",
    "        n = torch.cuda.device_count()\n",
    "        for i in range(n):\n",
    "            p = torch.cuda.get_device_properties(i)\n",
    "            print(f\"GPU {i}: {p.name} ({p.total_memory / 1e9:.1f}GB)\")\n",
    "        return torch.device(\"cuda:0\"), n > 1\n",
    "    return torch.device(\"cpu\"), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b51cf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int = 42) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d54e27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device, _multi = setup_device()\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a46ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainConfig:\n",
    "    rank: int = 16\n",
    "    alpha: Optional[int] = None\n",
    "    lr: float = 0.0005\n",
    "    batch_size: int = 4\n",
    "    grad_accum: int = 1\n",
    "    num_epochs: int = 10\n",
    "    num_workers: int = 4\n",
    "    caption_key: str = \"text\"\n",
    "    image_key: str = \"image\"\n",
    "    image_size: int = 512\n",
    "    save_dir: str = \"misc\"\n",
    "    save_every_steps: int = 0\n",
    "    target_text_encoder: bool = False\n",
    "    max_train_samples: int = 10000\n",
    "    prompts_for_eval: int = 50\n",
    "    guidance_scale: float = 7.5\n",
    "    num_inference_steps: int = 30\n",
    "    sd_model: str = \"runwayml/stable-diffusion-v1-5\"\n",
    "    prompts: str = \"sdlora_prompts.json\"\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.alpha is None:\n",
    "            self.alpha = self.rank * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7779224",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, original_layer: nn.Module, rank: int, alpha: int, dropout: float = 0.0, seed: int = 42) -> None:\n",
    "        super().__init__()\n",
    "        self.original_layer = original_layer\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.scale = alpha / rank\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0.0 else nn.Identity()\n",
    "        for p in self.original_layer.parameters(): p.requires_grad_(False)\n",
    "        if hasattr(original_layer, \"weight\"):\n",
    "            dtype = original_layer.weight.dtype\n",
    "            device = original_layer.weight.device\n",
    "        else:\n",
    "            dtype = torch.float32\n",
    "            device = next(original_layer.parameters(), torch.zeros(1)).device\n",
    "        g = torch.Generator().manual_seed(seed)\n",
    "        if isinstance(self.original_layer, nn.Linear):\n",
    "            in_features = self.original_layer.in_features; out_features = self.original_layer.out_features\n",
    "            self.lora_A = nn.Linear(in_features, rank, bias = False)\n",
    "            self.lora_B = nn.Linear(rank, out_features, bias = False)\n",
    "            nn.init.kaiming_uniform_(self.lora_A.weight, a = np.sqrt(5), generator = g)\n",
    "            nn.init.zeros_(self.lora_B.weight)\n",
    "            self.lora_A.to(device, dtype = dtype); self.lora_B.to(device, dtype = dtype)\n",
    "        elif isinstance(self.original_layer, nn.Conv2d):\n",
    "            self.lora_A = nn.Conv2d(self.original_layer.in_channels, rank, self.original_layer.kernel_size, bias = False)\n",
    "            self.lora_B = nn.Conv2d(rank, self.original_layer.out_channels, 1, stride = self.original_layer.stride, padding = self.original_layer.padding, dilation = self.original_layer.dilation, groups = self.original_layer.groups, bias = False)\n",
    "            nn.init.kaiming_uniform_(self.lora_A.weight, a = np.sqrt(5), generator = g)\n",
    "            nn.init.zeros_(self.lora_B.weight)\n",
    "            self.lora_A.to(device, dtype = dtype); self.lora_B.to(device, dtype = dtype)\n",
    "        else:\n",
    "            raise TypeError(f\"Unsupported layer type: {type(original_layer)}\")\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        base = self.original_layer(x); lora = self.lora_B(self.lora_A(self.dropout(x)))\n",
    "        return base + lora * self.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d07bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lora_to_unet(unet: nn.Module, rank: int, alpha: int, dropout: float = 0.1, target_modules: list[str] | None = None) -> dict[str, LoRALayer]:\n",
    "    if target_modules is None: \n",
    "        target_modules = [\"to_k\", \"to_q\", \"to_v\", \"to_out.0\", \"ff.net.0\", \"ff.net.2\", \"attn1.to_q\", \"attn1.to_v\"]\n",
    "    layers: dict[str, LoRALayer] = {}\n",
    "    def inject(module: nn.Module, prefix: str = \"\") -> None:\n",
    "        for name, child in module.named_children():\n",
    "            full = f\"{prefix}.{name}\" if prefix else name\n",
    "            if isinstance(child, LoRALayer): \n",
    "                continue\n",
    "            if any(full.endswith(t) for t in target_modules) and isinstance(child, (nn.Linear, nn.Conv2d)):\n",
    "                wrapped = LoRALayer(child, rank, alpha, dropout = dropout)\n",
    "                setattr(module, name, wrapped)\n",
    "                wrapped._is_lora = True\n",
    "                wrapped._lora_target_name = full\n",
    "                layers[full] = wrapped\n",
    "            else: \n",
    "                inject(child, full)\n",
    "    inject(unet)\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96acae12",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TransformData(Dataset):\n",
    "    dataset: datasets.Dataset\n",
    "    config: TrainConfig\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.image_transform = T.Compose([\n",
    "            T.Resize((self.config.image_size, self.config.image_size)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.5], [0.5])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.dataset), self.config.max_train_samples) if self.config.max_train_samples else len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        item = self.dataset[idx]\n",
    "        image = item[self.config.image_key]\n",
    "        if not isinstance(image, Image.Image):\n",
    "            image = Image.fromarray(image)\n",
    "        image = image.convert(\"RGB\")\n",
    "        image = self.image_transform(image)\n",
    "        \n",
    "        caption = item[self.config.caption_key]\n",
    "        if not isinstance(caption, str):\n",
    "            caption = str(caption)\n",
    "\n",
    "        return {\n",
    "            \"pixel_values\": image,\n",
    "            \"text\": caption\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995847a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_training(config: TrainConfig) -> dict[str, Any]:\n",
    "    device, _ = setup_device()\n",
    "    dtype = torch.float32\n",
    "    model_id = getattr(config, \"sd_model\", \"runwayml/stable-diffusion-v1-5\")\n",
    "    unet = UNet2DConditionModel.from_pretrained(model_id, subfolder=\"unet\", torch_dtype=dtype)\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(model_id, subfolder=\"tokenizer\")\n",
    "    text_encoder = CLIPTextModel.from_pretrained(model_id, subfolder=\"text_encoder\", torch_dtype=dtype)\n",
    "    scheduler = DDPMScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
    "    vae = AutoencoderKL.from_pretrained(model_id, subfolder=\"vae\", torch_dtype=dtype)\n",
    "\n",
    "    for m in [text_encoder, vae]:\n",
    "        for p in m.parameters():\n",
    "            p.requires_grad_(False)\n",
    "\n",
    "    lora_layers = lora_to_unet(unet, rank=config.rank if hasattr(config, \"rank\") else 16, alpha=config.alpha if hasattr(config, \"alpha\") else 32, dropout=getattr(config, \"dropout\", 0.1),)\n",
    "    unet = unet.to(device)\n",
    "    text_encoder = text_encoder.to(device)\n",
    "    vae = vae.to(device)\n",
    "\n",
    "    text_encoder.eval()\n",
    "    vae.eval()\n",
    "\n",
    "    return {\n",
    "        \"device\": device,\n",
    "        \"dtype\": dtype,\n",
    "        \"unet\": unet,\n",
    "        \"vae\": vae,\n",
    "        \"tokenizer\": tokenizer,\n",
    "        \"text_encoder\": text_encoder,\n",
    "        \"scheduler\": scheduler,\n",
    "        \"lora_layers\": lora_layers\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac9daa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch, tokenizer):\n",
    "    images = torch.stack([example[\"pixel_values\"] for example in batch])\n",
    "    captions = [example[\"text\"] for example in batch]\n",
    "    \n",
    "    tokens = tokenizer(\n",
    "        captions,\n",
    "        padding=\"max_length\",\n",
    "        max_length=tokenizer.model_max_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"pixel_values\": images,\n",
    "        \"input_ids\": tokens.input_ids,\n",
    "        \"attention_mask\": tokens.attention_mask,\n",
    "        \"captions\": captions,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc646dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_lora_state_dict(model: nn.Module) -> dict[str, torch.Tensor]:\n",
    "    state_dict = {}\n",
    "    for name, m in model.named_modules():\n",
    "        if isinstance(m, LoRALayer):\n",
    "            state_dict[f\"{name}.lora_A.weight\"] = m.lora_A.weight.detach().cpu()\n",
    "            state_dict[f\"{name}.lora_B.weight\"] = m.lora_B.weight.detach().cpu()\n",
    "            state_dict[f\"{name}.alpha\"] = torch.tensor(m.alpha)\n",
    "    return state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb1d083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_grad(func):\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        with torch.no_grad():\n",
    "            return func(*args, **kwargs)\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430e8901",
   "metadata": {},
   "outputs": [],
   "source": [
    "@no_grad\n",
    "def encode_inputs(vae, text_encoder, pixel_values, input_ids, attention_mask):\n",
    "    latents = vae.encode(pixel_values).latent_dist.sample() * 0.18215\n",
    "    encoder_hidden_states = text_encoder(input_ids, attention_mask=attention_mask)[0]\n",
    "    return latents, encoder_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92127510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LoRA(config: TrainConfig, train_dataloader: DataLoader, setup: dict[str, Any], output_dir: str | None = None, save_every: int = 500) -> tuple[list[float], dict[str, Any]]:\n",
    "    if output_dir is None: \n",
    "        output_dir = config.save_dir\n",
    "\n",
    "    device, dtype = setup[\"device\"], torch.float32\n",
    "    unet, vae, text_encoder = setup[\"unet\"], setup[\"vae\"], setup[\"text_encoder\"]\n",
    "    scheduler, lora_layers = setup[\"scheduler\"], setup[\"lora_layers\"]\n",
    "\n",
    "    set_mpl_params(); seed_everything(42); os.makedirs(output_dir, exist_ok = True)\n",
    "\n",
    "    lora_params: list[nn.Parameter] = []\n",
    "    for lora_layer in lora_layers.values():\n",
    "        lora_params.extend(list(lora_layer.lora_A.parameters())); lora_params.extend(list(lora_layer.lora_B.parameters()))\n",
    "\n",
    "    optimizer = torch.optim.AdamW(params = lora_params, lr = config.lr, weight_decay = 0.0, betas = (0.9, 0.999), eps = 1e-8)\n",
    "    total_steps = len(train_dataloader) * config.num_epochs; warmup_steps = int(total_steps * 0.01)\n",
    "    lr_scheduler = get_scheduler(name = \"constant_with_warmup\", optimizer = optimizer, num_warmup_steps = warmup_steps, num_training_steps = total_steps)\n",
    "\n",
    "    global_step, loss_history, epoch_times, epoch_memory = 0, [], [], []\n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.reset_peak_memory_stats(device)\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    for epoch in range(config.num_epochs):\n",
    "        unet.train(); epoch_start = time.time(); epoch_loss = 0.0; ema_loss = None\n",
    "        bar = tqdm(train_dataloader, desc = f\"Epoch {epoch + 1}/{config.num_epochs}\")\n",
    "        for batch in bar:\n",
    "            pixel_values = batch[\"pixel_values\"].to(device, dtype = dtype); input_ids = batch[\"input_ids\"].to(device); attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            latents, encoder_hidden_states = encode_inputs(vae, text_encoder, pixel_values, input_ids, attention_mask)\n",
    "            bsz = latents.shape[0]; timesteps = torch.randint(0, scheduler.num_train_timesteps, (bsz,), device = device).long(); noise = torch.randn_like(latents); noisy_latents = scheduler.add_noise(latents, noise, timesteps)\n",
    "            model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "            pred_type = getattr(getattr(scheduler, \"config\", None), \"prediction_type\", \"epsilon\")\n",
    "            if pred_type == \"epsilon\": \n",
    "                target = noise\n",
    "            elif pred_type == \"v_prediction\": \n",
    "                target = scheduler.get_velocity(latents, noise, timesteps)\n",
    "            else: \n",
    "                raise ValueError(f\"Unknown scheduler prediction type: {pred_type}\")\n",
    "            loss = F.mse_loss(model_pred.float(), target.float())\n",
    "            optimizer.zero_grad(set_to_none = True); loss.backward(); torch.nn.utils.clip_grad_norm_(lora_params, 1.0); optimizer.step(); lr_scheduler.step()\n",
    "            global_step += 1; loss_history.append(loss.item()); epoch_loss += loss.item()\n",
    "            ema_loss = loss.item() if ema_loss is None else 0.9 * ema_loss + 0.1 * loss.item()\n",
    "            bar.set_postfix({\"loss\": f\"{loss.item():.4f}\", \"ema\": f\"{ema_loss:.4f}\"})\n",
    "            if global_step % save_every == 0:\n",
    "                save_path = os.path.join(output_dir, f\"lora_step_{global_step}_r{config.rank}.pt\"); torch.save(extract_lora_state_dict(unet), save_path)\n",
    "        epoch_end = time.time(); duration_min = (epoch_end - epoch_start) / 60\n",
    "        epoch_times.append(duration_min); peak_mem = torch.cuda.max_memory_allocated(device) / 1e9 if torch.cuda.is_available() else 0.0; epoch_memory.append(peak_mem)\n",
    "        avg_loss = epoch_loss / len(train_dataloader)\n",
    "        print(f\"\\nEpoch {epoch + 1}: avg_loss = {avg_loss:.4f} | time = {duration_min:.2f}m | peak_mem = {peak_mem:.2f}GB\")\n",
    "        rank_dir = os.path.join(output_dir, f\"rank_{config.rank}\"); os.makedirs(rank_dir, exist_ok = True)\n",
    "        if (epoch + 1) % 2 == 0:\n",
    "            plt.figure(figsize = (9, 6)); plt.plot(loss_history); plt.title(f\"Training loss at rank {config.rank}\"); plt.xlabel(\"Steps\"); plt.ylabel(\"MSE loss\"); plt.grid(True); plt.tight_layout(); plt.savefig(f\"{rank_dir}/loss_epoch_{epoch + 1}.png\", dpi = 300); plt.close()\n",
    "        if torch.cuda.is_available(): torch.cuda.reset_peak_memory_stats(device)\n",
    "\n",
    "    save_path = os.path.join(output_dir, f\"lora_final_r{config.rank}.pt\"); torch.save(extract_lora_state_dict(unet), save_path)\n",
    "    elapsed_time = time.perf_counter() - start_time; file_size_mb = os.path.getsize(save_path) / 1024 ** 2\n",
    "    stats = {\"rank\": config.rank, \"train_time_s\": elapsed_time, \"peak_mem_gb\": max(epoch_memory) if epoch_memory else 0, \"file_size_mb\": file_size_mb, \"final_loss\": loss_history[-1] if loss_history else None}\n",
    "    print(f\"Rank {config.rank} Summary → time = {elapsed_time:.1f}s | peak_mem = {stats['peak_mem_gb']:.2f}GB | file = {file_size_mb:.2f}MB\")\n",
    "    return loss_history, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb3b43c-1de6-4a3c-a185-d4be2e96e279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_cuda():\n",
    "    import torch, gc\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56862fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TrainConfig()\n",
    "train_dataset = TransformData(dataset, config)\n",
    "all_stats = {}\n",
    "all_losses = {}\n",
    "clean_cuda()\n",
    "\n",
    "for rank in [16, 64]:\n",
    "    config = TrainConfig(rank = rank)\n",
    "    setup = setup_training(config)\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size = config.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers = config.num_workers,\n",
    "        collate_fn = lambda ex: collate_fn(ex, setup[\"tokenizer\"])\n",
    "    )\n",
    "\n",
    "    loss_history, stats = train_LoRA(config, train_dataloader, setup)\n",
    "    all_stats[rank] = stats\n",
    "    all_losses[rank] = loss_history\n",
    "\n",
    "print(\"\\nTraining summary by rank:\")\n",
    "for r, s in all_stats.items():\n",
    "    print(f\"Rank {r}: final_loss={s['final_loss']:.4f}, time={s['train_time_s']:.1f}s, file_size={s['file_size_mb']:.2f}MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b638ef8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_mpl_params()\n",
    "plt.figure(figsize = (8, 5))\n",
    "colors = [\"#0d47a1\", \"#64b5f6\"] \n",
    "\n",
    "for (rank, losses), c in zip(sorted(all_losses.items()), colors):\n",
    "    smooth = np.convolve(losses, np.ones(50) / 50, mode = \"valid\")\n",
    "    plt.plot(smooth, label = f\"Rank {rank}\", linewidth = 2, color = c)\n",
    "\n",
    "plt.title(\"Smoothed loss curves for different ranks\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Smoothed loss\")\n",
    "plt.legend(title = \"Rank\")\n",
    "plt.grid(True, linestyle = \"--\", alpha = 0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"misc/loss_curves_smooth.png\", dpi = 150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9823dc61-e5fd-41e2-87bd-6f33318d86ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_mpl_params()\n",
    "ranks = sorted(all_stats.keys())\n",
    "times = [all_stats[r][\"train_time_s\"] / 60 for r in ranks]\n",
    "mems = [all_stats[r][\"peak_mem_gb\"] for r in ranks]\n",
    "sizes = [all_stats[r][\"file_size_mb\"] for r in ranks]\n",
    "\n",
    "plt.figure(figsize = (8, 5))\n",
    "plt.plot(ranks, times, marker = \"o\", linewidth = 2, color = \"#1f77b4\", label = \"Time to train (min)\")\n",
    "plt.plot(ranks, mems, marker = \"s\", linewidth = 2, color = \"#2980b9\", label = \"Peak memory (GB)\")\n",
    "plt.plot(ranks, sizes, marker = \"D\", linewidth = 2, color = \"#5dade2\", label = \"File size (MB)\")\n",
    "\n",
    "plt.title(\"Performance of each rank\")\n",
    "plt.xlabel(\"Rank\")\n",
    "plt.ylabel(\"Given value\")\n",
    "plt.xticks(ranks)\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle = \"--\", alpha = 0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"misc/performance_vs_rank.png\", dpi = 150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854c2f34-2556-492a-9e15-25e80d2d6116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lora_weights(unet: UNet2DConditionModel, config: TrainConfig, lora_weights_path: str) -> UNet2DConditionModel:\n",
    "    state_dict = torch.load(lora_weights_path, map_location = \"cpu\", weights_only = True)\n",
    "    lora_layers = lora_to_unet(unet, rank = config.rank, alpha = config.alpha)\n",
    "    for name, layer in lora_layers.items():\n",
    "        if f\"{name}.lora_A.weight\" in state_dict:\n",
    "            layer.lora_A.weight.data.copy_(state_dict[f\"{name}.lora_A.weight\"])\n",
    "        if f\"{name}.lora_B.weight\" in state_dict:\n",
    "            layer.lora_B.weight.data.copy_(state_dict[f\"{name}.lora_B.weight\"])\n",
    "    return unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f25f67d-e3da-4ba0-accb-d2f0ba00ca6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(config: TrainConfig, lora_paths: list[tuple[int, str]], categories: list[str] = [\"normal\", \"regular_abstract\"], load_lora: bool = True) -> None:\n",
    "    seed_everything(42); device, dtype = setup_device()[0], torch.float32\n",
    "    with open(config.prompts, \"r\") as f: data = json.load(f)\n",
    "    for category in categories:\n",
    "        prompts = data[category][: config.prompts_for_eval]\n",
    "        for rank, lora_path in lora_paths:\n",
    "            pipe = StableDiffusionPipeline.from_pretrained(config.sd_model, safety_checker = None, requires_safety_checker = False, torch_dtype = dtype).to(device)\n",
    "            if load_lora:\n",
    "                cfg = TrainConfig(rank = rank, alpha = rank * 2)\n",
    "                pipe.unet = load_lora_weights(pipe.unet, cfg, lora_path)\n",
    "            pipe.unet.eval(); pipe.text_encoder.eval(); pipe.vae.eval()\n",
    "            out_dir = os.path.join(config.save_dir, f\"generated_rank_{rank}_{category}_lora\" if load_lora else f\"generated_baseline_{category}\"); os.makedirs(out_dir, exist_ok = True)\n",
    "            print(f\"\\nGenerating {len(prompts)} images to {out_dir}\")\n",
    "            for i, prompt in enumerate(prompts):\n",
    "                g = torch.Generator(device = device).manual_seed(1234 + i)\n",
    "                image = pipe(prompt, num_inference_steps = config.num_inference_steps, guidance_scale = config.guidance_scale, generator = g).images[0]\n",
    "                name = (f\"rank{rank}_{category}_{i:03d}.png\" if load_lora else f\"baseline_{i:03d}.png\"); image.save(os.path.join(out_dir, name))\n",
    "            print(f\"Saved {len(prompts)} images to {out_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e6c002-f4a7-425b-bd1d-1321fecc9147",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_images(config, lora_paths = [(16, \"misc/lora_final_r16.pt\"), (64, \"misc/lora_final_r64.pt\")], categories = [\"normal\", \"regular_abstract\"], load_lora = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb15498-6468-4bba-8b26-f034f44a1933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comparison_image(img1: Image.Image, img2: Image.Image, prompt: str, img_size: Tuple[int, int] = (512, 512)) -> Tuple[Image.Image, Image.Image, Image.Image]:\n",
    "    from PIL import Image, ImageDraw, ImageFont\n",
    "    img1 = img1.resize(img_size)\n",
    "    img2 = img2.resize(img_size)\n",
    "    canvas_width = img_size[0] * 2 + 20\n",
    "    canvas_height = img_size[1] + 50\n",
    "    canvas = Image.new(\"RGB\", (canvas_width, canvas_height), \"white\")\n",
    "    canvas.paste(img1, (0, 0))\n",
    "    canvas.paste(img2, (img_size[0] + 20, 0))\n",
    "    draw = ImageDraw.Draw(canvas)\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", 14)\n",
    "    except:\n",
    "        font = ImageFont.load_default()\n",
    "    \n",
    "    text = prompt[:100] + \"...\" if len(prompt) > 100 else prompt\n",
    "    text_bbox = draw.textbbox((0, 0), text, font=font)\n",
    "    text_width = text_bbox[2] - text_bbox[0]\n",
    "    draw.text(((canvas_width - text_width) // 2, img_size[1] + 15), text, fill = \"black\", font = font)\n",
    "    \n",
    "    return img1, img2, canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db423f35-1f50-4d0a-ae25-b11aea8b6894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparative_test(config: TrainConfig, lora_paths: list[tuple[int, str]], categories: list[str] = [\"normal\", \"regular_abstract\"], output_dir: str = \"outputs\") -> None:\n",
    "    device, _ = setup_device(); seed_everything(42); set_mpl_params(); os.makedirs(output_dir, exist_ok = True)\n",
    "    with open(config.prompts, \"r\") as f: data = json.load(f)\n",
    "    for category in categories:\n",
    "        prompts = data[category][: config.prompts_for_eval]\n",
    "        for rank, lora_path in lora_paths:\n",
    "            pipe = StableDiffusionPipeline.from_pretrained(config.sd_model, torch_dtype = torch.float16, safety_checker = None, requires_safety_checker = False).to(device)\n",
    "            pipe.enable_attention_slicing()\n",
    "            base_imgs = []\n",
    "            for i, prompt in enumerate(prompts):\n",
    "                g = torch.Generator(device = device).manual_seed(1234 + i)\n",
    "                img = pipe(prompt, num_inference_steps = config.num_inference_steps, guidance_scale = config.guidance_scale, generator = g).images[0]\n",
    "                base_imgs.append(img)\n",
    "            cfg = TrainConfig(rank = rank, alpha = rank * 2)\n",
    "            pipe.unet = load_lora_weights(pipe.unet, cfg, lora_path).to(device)\n",
    "            torch.cuda.empty_cache()\n",
    "            for i, (prompt, base_img) in enumerate(zip(prompts, base_imgs)):\n",
    "                g = torch.Generator(device = device).manual_seed(1234 + i)\n",
    "                lora_img = pipe(prompt, num_inference_steps = config.num_inference_steps, guidance_scale = config.guidance_scale, generator = g).images[0]\n",
    "                base_img, lora_img, comparison = create_comparison_image(base_img, lora_img, prompt)\n",
    "                fname = prompt.replace(\" \", \"_\")[:40]\n",
    "                rank_dir = os.path.join(output_dir, f\"rank_{rank}_{category}\"); os.makedirs(rank_dir, exist_ok = True)\n",
    "                base_img.save(f\"{rank_dir}/base_{i:02d}_{fname}.png\"); lora_img.save(f\"{rank_dir}/lora_{i:02d}_{fname}.png\"); comparison.save(f\"{rank_dir}/comparison_{i:02d}_{fname}.png\")\n",
    "                plt.figure(figsize = (12, 6)); plt.imshow(comparison); plt.axis(\"off\"); plt.title(f\"Comparison {i+1}\"); plt.show()\n",
    "            print(f\"Saved comparisons for rank {rank}, category '{category}' to {rank_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848501c2-4ead-49e9-8198-d329067acc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TrainConfig()\n",
    "lora_paths = [(16, \"misc/lora_final_r16.pt\"), (64, \"misc/lora_final_r64.pt\")]\n",
    "comparative_test(config, lora_paths = lora_paths, categories = [\"normal\", \"regular_abstract\"], output_dir = \"comparisons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eed6685-4099-4956-b590-5c5f97f3dec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_clip_score(config: TrainConfig, rank: int, category: str) -> list[float]:\n",
    "    clip_score_fn = partial(clip_score, model_name_or_path = \"openai/clip-vit-base-patch16\")\n",
    "    with open(config.prompts, \"r\") as f: data = json.load(f)\n",
    "    prompts = data[category][: config.prompts_for_eval]\n",
    "    folder = Path(config.save_dir) / f\"generated_rank_{rank}_{category}_lora\"\n",
    "    images = sorted(folder.glob(\"*.png\"))\n",
    "    scores = []\n",
    "    for i, (path, prompt) in enumerate(zip(images, prompts)):\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        img_tensor = torch.from_numpy(np.array(img)).permute(2, 0, 1).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            score = clip_score_fn(img_tensor, prompt)\n",
    "            scores.append(score.item())\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d967f3af-3e27-45ed-9c65-db5b5229fe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TrainConfig()\n",
    "ranks = [16, 64]\n",
    "categories = [\"normal\", \"regular_abstract\"]\n",
    "\n",
    "for category in categories:\n",
    "    for rank in ranks:\n",
    "        scores = calculate_clip_score(config, rank, category)\n",
    "        print(f\"Rank {rank} | Category {category} → Mean CLIP: {np.mean(scores):.4f}, Std: {np.std(scores):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
